{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.   3. ]\n",
      " [-0.   2.   0.1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "test_mask = (X>0)\n",
    "print(test_mask*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.  18.  33. ]\n",
      " [ 9.  22.  30.1]]\n"
     ]
    }
   ],
   "source": [
    "test_X_qqq = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "test_B_qqq = np.array([10, 20, 30])\n",
    "print(test_X_qqq + test_B_qqq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False  True]\n",
      " [ True False False]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "test_X_qqq = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "test_Y_qqq = np.array([[1, 2, 3],\n",
    "              [-1, 100, 0.5]\n",
    "              ])\n",
    "print(test_X_qqq == test_Y_qqq)\n",
    "print(np.sum(test_X_qqq == test_Y_qqq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.217215, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.155566, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.101459, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.286570, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.170695, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.067441, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.367471, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.110023, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226594, Train accuracy: 0.207667, val accuracy: 0.215000\n",
      "Loss: 2.068688, Train accuracy: 0.230111, val accuracy: 0.235000\n",
      "Loss: 2.120957, Train accuracy: 0.256444, val accuracy: 0.258000\n",
      "Loss: 2.110639, Train accuracy: 0.268778, val accuracy: 0.268000\n",
      "Loss: 2.121036, Train accuracy: 0.282556, val accuracy: 0.287000\n",
      "Loss: 1.999364, Train accuracy: 0.300111, val accuracy: 0.306000\n",
      "Loss: 2.058220, Train accuracy: 0.322222, val accuracy: 0.325000\n",
      "Loss: 1.853882, Train accuracy: 0.356778, val accuracy: 0.360000\n",
      "Loss: 1.857394, Train accuracy: 0.388889, val accuracy: 0.381000\n",
      "Loss: 2.045495, Train accuracy: 0.403778, val accuracy: 0.394000\n",
      "Loss: 1.975791, Train accuracy: 0.424000, val accuracy: 0.414000\n",
      "Loss: 1.779497, Train accuracy: 0.449889, val accuracy: 0.440000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f40f2bfe898>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.275985, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.347515, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264348, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287890, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253264, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265011, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212698, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.320777, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302624, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.311032, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234316, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283390, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.353994, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.217454, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.378711, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256101, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309860, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264767, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276485, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274562, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.329673, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 1.436811, Train accuracy: 0.423444, val accuracy: 0.409000\n",
      "Loss: 1.808227, Train accuracy: 0.569778, val accuracy: 0.566000\n",
      "Loss: 1.541544, Train accuracy: 0.618889, val accuracy: 0.625000\n",
      "Loss: 1.078942, Train accuracy: 0.667000, val accuracy: 0.665000\n",
      "Loss: 1.392780, Train accuracy: 0.687000, val accuracy: 0.651000\n",
      "Loss: 1.365402, Train accuracy: 0.699000, val accuracy: 0.672000\n",
      "Loss: 1.088524, Train accuracy: 0.733778, val accuracy: 0.694000\n",
      "Loss: 0.663420, Train accuracy: 0.738333, val accuracy: 0.704000\n",
      "Loss: 1.474907, Train accuracy: 0.773778, val accuracy: 0.717000\n",
      "Loss: 0.647732, Train accuracy: 0.775556, val accuracy: 0.706000\n",
      "Loss: 0.607067, Train accuracy: 0.779111, val accuracy: 0.710000\n",
      "Loss: 0.909896, Train accuracy: 0.793333, val accuracy: 0.722000\n",
      "Loss: 0.830649, Train accuracy: 0.822111, val accuracy: 0.727000\n",
      "Loss: 1.199751, Train accuracy: 0.806444, val accuracy: 0.706000\n",
      "Loss: 0.577387, Train accuracy: 0.832333, val accuracy: 0.739000\n",
      "Loss: 0.761632, Train accuracy: 0.816111, val accuracy: 0.731000\n",
      "Loss: 0.674111, Train accuracy: 0.831667, val accuracy: 0.730000\n",
      "Loss: 0.413585, Train accuracy: 0.857333, val accuracy: 0.728000\n",
      "Loss: 0.687732, Train accuracy: 0.860889, val accuracy: 0.740000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-4)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-2, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.351801, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319693, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.295926, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.345443, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310013, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326544, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.255641, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.199259, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.164561, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.025661, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.883798, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.263931, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.662918, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.245522, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.943906, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.663953, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.042200, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.418303, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.519709, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.823944, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.022291, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.585656, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.937038, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.908215, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.233499, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.095108, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.718242, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.162756, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.015933, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.363952, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.842670, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.657567, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.388463, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.911120, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.693765, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 2.010436, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.115900, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.613706, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.738574, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.885192, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.474042, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.119274, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.672047, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.767860, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.478802, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.926619, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.337383, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.683186, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.660081, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.401881, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.635264, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.635617, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.622938, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.854111, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.556636, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.634732, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.076198, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.007559, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.531114, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.041336, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.022564, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.647379, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.369538, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.379903, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.986917, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.318972, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.283966, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.936269, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.196061, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.739352, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.634901, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.201189, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.582981, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.742143, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.293135, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.748190, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.976545, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.357876, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.992895, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.602046, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.602167, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.173057, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.312798, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.564551, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.961713, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.274091, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.304703, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.958926, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.377168, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.657193, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.389767, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.576803, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.556738, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.175409, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.322616, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.251510, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.060407, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.661861, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.227308, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.597969, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.887250, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.338688, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.842966, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.160604, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.347215, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.223119, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.436648, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.535320, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.249121, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.248964, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.549803, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.259687, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.097239, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.324286, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.315431, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.420746, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.443017, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.301618, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.533658, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.607993, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.239065, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.649177, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.174445, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.602848, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.363682, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.160640, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.197888, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.126413, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.008120, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.643235, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.592807, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.412599, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.212850, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.196541, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.280346, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.403204, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.229377, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.400568, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.391042, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.102808, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.445285, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.406323, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.219193, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.449330, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.304464, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.494726, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.327706, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.361608, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.460816, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.424061, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.291153, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.278395, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.192048, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.033070, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.730578, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.091029, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.071398, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.784605, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.844748, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.686709, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 2.037427, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.949546, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.644895, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.677069, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.880372, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.902955, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.454495, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.846028, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.224538, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.193286, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-10)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=0.3, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-571de9d157c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMomentumSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best validation accuracy achieved: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10,/\n",
    "                    hidden_layer_size = hidden_layer_size, reg = reg_strength)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=learning_rates,/\n",
    "                  learning_rate_decay=learning_rate_decay, num_epochs=num_epochs, batch_size=batch_size)\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "best_classifier = model\n",
    "best_val_accuracy = np.max(val_history)\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
